{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from chat_dataset import ChatDataset\n",
    "from chat_dataloader import ChatDataLoader\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'skt/ko-gpt-trinity-1.2B-v0.5'\n",
    "file_path = '/home/yys/COMU-ChatBot/data_final.tsv'\n",
    "output_dir = '/home/yys/COMU-ChatBot/gpt/output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 및 모델 로딩\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--model_checkpoint MODEL_CHECKPOINT]\n",
      "                             [--max_input_length MAX_INPUT_LENGTH]\n",
      "                             [--max_target_length MAX_TARGET_LENGTH]\n",
      "                             [--train_batch_size TRAIN_BATCH_SIZE]\n",
      "                             [--num_train_epochs NUM_TRAIN_EPOCHS] [--lr LR]\n",
      "                             [--wd WD] [--model_name MODEL_NAME]\n",
      "                             [--base_path BASE_PATH] [--model_path MODEL_PATH]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/yys/.local/share/jupyter/runtime/kernel-8596848d-3d6c-4c30-8a6c-71bb3f74664d.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yys/.conda/envs/chatbot-gpt/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3468: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def gpt_train(args):\n",
    "    # model  = AutoModelForCausalLM.from_pretrained(args.model_checkpoint)\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(args.model_checkpoint, use_fast=True, stride=128)\n",
    "    \n",
    "    model.config.max_length = args.max_target_length\n",
    "    tokenizer.model_max_length = args.max_target_length\n",
    "    return model, tokenizer\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Hyperparameter for Training gpt-based model for conditional generation')\n",
    "    parser.add_argument('--model_checkpoint', default='skt/ko-gpt-trinity-1.2B-v0.5', type=str,\n",
    "                        help='huggingface model name to train')\n",
    "    parser.add_argument('--max_input_length', default=100, type=int,\n",
    "                        help='max input length for dialog')\n",
    "    parser.add_argument('--max_target_length', default=100, type=int,\n",
    "                    help='max target length for dialog')\n",
    "    parser.add_argument('--train_batch_size', default=8, type=int,\n",
    "                                            help='train batch size')\n",
    "    parser.add_argument('--num_train_epochs', default=1, type=int,\n",
    "                                            help='train epoch size')\n",
    "    parser.add_argument('--lr', default=5e-5, type=int,\n",
    "                                            help='learning rate for training')\n",
    "    parser.add_argument('--wd', default=0.01, type=int,\n",
    "                                            help='weight decay for training'),\n",
    "    parser.add_argument('--model_name', default='skt/ko-gpt-trinity-1.2B-v0.5', type=str,\n",
    "                                            help='model name for saving')\n",
    "    parser.add_argument('--base_path', default='../data/data_final.tsv', type=str,\n",
    "                                                help='dataset path')\n",
    "    parser.add_argument('--model_path', default='./output', type=str,\n",
    "                                                help='model path for saving')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    #Load dataset\n",
    "    dataset = pd.read_csv(args.base_path,sep='\\t')\n",
    "    tokenized_dataset = ChatDataset(dataset)\n",
    "    batched_dataset = ChatDataset(tokenized_dataset, args.train_batch_size)\n",
    "    model = AutoModelForCausalLM.from_pretrained(args.model_checkpoint)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_checkpoint, use_fast=True, stride=128)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir = args.model_path,\n",
    "        evaluation_strategy='epoch',\n",
    "        save_total_limit=1,\n",
    "        num_train_epochs=args.num_train_epochs,\n",
    "        per_device_train_batch_size=args.train_batch_size,\n",
    "        save_steps=1000,\n",
    "        learning_rate=args.lr,\n",
    "        weight_decay=args.wd,\n",
    "        warmup_steps=3000,\n",
    "        logging_steps=1000,\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='loss',\n",
    "        greater_is_better=False     \n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model = model,\n",
    "        args=training_args,\n",
    "        train_dataset=batched_dataset,\n",
    "        data_collator=ChatDataLoader.collate_fn,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    # Training\n",
    "    print('Start Training...')  \n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    # Saving model\n",
    "    print('Saving Model...')\n",
    "    trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../data/data_final.tsv',sep='\\t')\n",
    "tokenized_dataset = ChatDataset(dataset)\n",
    "batched_dataset = ChatDataLoader(tokenized_dataset, 4)\n",
    "model = AutoModelForCausalLM.from_pretrained('skt/ko-gpt-trinity-1.2B-v0.5')\n",
    "tokenizer = AutoTokenizer.from_pretrained('skt/ko-gpt-trinity-1.2B-v0.5', use_fast=True, stride=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 50\n",
      "<class 'numpy.ndarray'> 50\n",
      "<class 'list'> 50\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(type(tokenized_dataset[0][i]), len(tokenized_dataset[0][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7692"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batched_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yys/COMU-ChatBot/gpt/chat_dataloader.py:14: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  return torch.LongTensor(data), torch.LongTensor(mask), torch.LongTensor(label)\n",
      "/home/yys/COMU-ChatBot/gpt/chat_dataloader.py:14: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  return torch.LongTensor(data), torch.LongTensor(mask), torch.LongTensor(label)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ids: tensor([[    2, 37215, 27248, 22912, 30930, 41095, 43781, 30195, 30021, 27212,\n",
      "         30476, 35891, 20476, 18904, 33252,   407,     4, 31927, 25837, 20476,\n",
      "         34327, 25768, 30143, 30021, 33333, 29979, 30086, 39999, 30930,     1,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    2, 39032, 18808, 32730, 30086, 47631, 46923, 46435,   407,     4,\n",
      "         30096, 24660, 37731, 29392,     1,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3]])\n",
      "Mask: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]])\n",
      "Labels ids: tensor([[    9,     9,     9,     9,     9,     9,     9,     9,     9,     9,\n",
      "             9,     9,     9,     9,     9,     9, 31927, 25837, 20476, 34327,\n",
      "         25768, 30143, 30021, 33333, 29979, 30086, 39999, 30930,     1,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3],\n",
      "        [    9,     9,     9,     9,     9,     9,     9,     9,     9, 30096,\n",
      "         24660, 37731, 29392,     1,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3]])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    token_ids, mask, labels_ids = batch\n",
    "    print(\"Token ids:\", token_ids)\n",
    "    print(\"Mask:\", mask)\n",
    "    print(\"Labels ids:\", labels_ids)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot-gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
